import os
import sys
import torch
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    GPTNeoXForCausalLM
)

class JapaneseChatbot:
    def __init__(self, model_id="rinna/japanese-gpt-neox-3.6b", hf_token=None):
        """
        Initialize with Rinna's Japanese GPT-NeoX model.
        
        Args:
            model_id (str): Model identifier
            hf_token (str): Hugging Face authentication token
        """
        if not hf_token:
            raise ValueError("Hugging Face í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        print("ğŸ–¥ï¸ CPU ìµœì í™” ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë”©...")

        try:
            # Specific tokenizer loading for Rinna model
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id, 
                trust_remote_code=True,
                use_fast=False  # Disable fast tokenizer
            )

            # Model loading with specific configurations
            self.model = GPTNeoXForCausalLM.from_pretrained(
                model_id, 
                torch_dtype=torch.float32,  # Ensure CPU compatibility
                low_cpu_mem_usage=True
            )

            print(f"âœ… {model_id} ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print(f"ğŸ’» ì‚¬ìš© ì¤‘ì¸ CPU: {os.popen('wmic cpu get name').read().strip()}")

        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            sys.exit(1)

    def generate_response(self, prompt, max_tokens=256):
        """
        Generate response with improved tokenization handling.
        
        Args:
            prompt (str): User input
            max_tokens (int): Maximum tokens to generate
        
        Returns:
            str: Generated response
        """
        # Ensure pad_token is set
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        full_prompt = f"### æŒ‡ç¤º\n{prompt}\n### å›ç­”\n"
        
        try:
            # Tokenize input with padding and attention mask
            inputs = self.tokenizer(
                full_prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512  # Adjust as needed
            )
            
            # Generate with more robust settings
            generation_kwargs = {
                'input_ids': inputs.input_ids,
                'attention_mask': inputs.attention_mask,
                'max_new_tokens': max_tokens,
                'do_sample': True,
                'temperature': 0.7,
                'top_p': 0.9,
                'repetition_penalty': 1.2,
                'pad_token_id': self.tokenizer.pad_token_id
            }
            
            # Generate response
            outputs = self.model.generate(**generation_kwargs)
            
            # Decode response
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.split('\n### å›ç­”')[-1].strip()
            
            return response
        
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    def chat(self):
        """
        Interactive chat loop optimized for CPU
        """
        print("ğŸ¤– CPU ìµœì í™” ì¼ë³¸ì–´ ì±—ë´‡: ì•ˆë…•í•˜ì„¸ìš”!")
        print("ğŸ’¡ Rinna GPT-NeoX ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        
        while True:
            try:
                user_input = input("ë‹¹ì‹ : ").strip()
                
                if user_input.lower() in ['exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°']:
                    print("ğŸ¤– ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                    break
                
                # Generate and print response
                response = self.generate_response(user_input)
                print("ğŸ¤–:", response)
            
            except KeyboardInterrupt:
                print("\nì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

def main():
    # Get Hugging Face token
    HF_TOKEN = "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"
    
    if not HF_TOKEN:
        print("Hugging Face í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        HF_TOKEN = input("Hugging Face í† í°ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    try:
        # Use Rinna's Japanese GPT-NeoX model
        chatbot = JapaneseChatbot(
            model_id="rinna/japanese-gpt-neox-3.6b", 
            hf_token=HF_TOKEN
        )
        chatbot.chat()
    
    except Exception as e:
        print(f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main()