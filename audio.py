import os
import sys
import torch
import speech_recognition as sr
import pyttsx3
import threading
import queue
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    GPTNeoXForCausalLM
)

class JapaneseChatbot:
    def __init__(self, model_id="rinna/japanese-gpt-neox-3.6b", hf_token=None):
        """
        Initialize chatbot with continuous audio support for headphones.
        
        Args:
            model_id (str): Model identifier
            hf_token (str): Hugging Face authentication token
        """
        if not hf_token:
            raise ValueError("Hugging Face í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # Audio setup
        try:
            # Speech Recognition setup
            self.recognizer = sr.Recognizer()
            self.microphone = sr.Microphone()
            
            # Text-to-Speech setup
            self.tts_engine = pyttsx3.init()
            
            # Configure TTS for Japanese
            self.tts_engine.setProperty('rate', 150)  # Speed of speech
            voices = self.tts_engine.getProperty('voices')
            
            # Try to find a Japanese voice
            japanese_voices = [
                voice for voice in voices 
                if 'japanese' in voice.name.lower() or 'japan' in voice.name.lower()
            ]
            
            if japanese_voices:
                self.tts_engine.setProperty('voice', japanese_voices[0].id)
            
            # Queue for managing audio input/output
            self.audio_queue = queue.Queue()
            self.stop_listening = None
            
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            sys.exit(1)

        print("ğŸ–¥ï¸ CPU ìµœì í™” ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë”©...")

        try:
            # Tokenizer and model loading
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id, 
                trust_remote_code=True,
                use_fast=False
            )

            self.model = GPTNeoXForCausalLM.from_pretrained(
                model_id, 
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True
            )

            print(f"âœ… {model_id} ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            sys.exit(1)

    def listen_for_audio(self):
        """
        Continuous audio listening thread
        """
        while True:
            try:
                with self.microphone as source:
                    print("ğŸ¤ ë“£ê³  ìˆìŠµë‹ˆë‹¤... (ë§ì”€í•´ ì£¼ì„¸ìš”)")
                    self.recognizer.adjust_for_ambient_noise(source, duration=0.5)
                    audio = self.recognizer.listen(source)
                    
                    # Add audio to queue for processing
                    self.audio_queue.put(audio)
            
            except Exception as e:
                print(f"ìŒì„± ìˆ˜ì‹  ì˜¤ë¥˜: {e}")
                break

    def process_audio_queue(self):
        """
        Process audio queue and generate responses
        """
        while True:
            try:
                # Wait for audio in queue
                audio = self.audio_queue.get()
                
                try:
                    # Recognize speech (Japanese)
                    text = self.recognizer.recognize_google(audio, language='ja-JP')
                    print(f"ğŸ”Š ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
                    
                    # Check for exit commands
                    if text.lower() in ['exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°', 'quit']:
                        print("ğŸ¤– ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    
                    # Generate response
                    response = self.generate_response(text)
                    print("ğŸ¤–:", response)
                    
                    # Speak response
                    self.tts_engine.say(response)
                    self.tts_engine.runAndWait()
                
                except sr.UnknownValueError:
                    print("ğŸ¤· ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except sr.RequestError as e:
                    print(f"ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
            
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def start_audio_chat(self):
        """
        Start continuous audio chat
        """
        print("ğŸ¤– í—¤ë“œí° ëª¨ë“œ ì¼ë³¸ì–´ ì±—ë´‡: ì•ˆë…•í•˜ì„¸ìš”!")
        print("ğŸ’¡ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.")
        print("ğŸ’¡ 'ì¢…ë£Œ' ë˜ëŠ” 'exit'ë¡œ ëŒ€í™” ì¢…ë£Œ ê°€ëŠ¥")

        # Create threads for listening and processing
        listener_thread = threading.Thread(target=self.listen_for_audio, daemon=True)
        processor_thread = threading.Thread(target=self.process_audio_queue, daemon=True)
        
        # Start threads
        listener_thread.start()
        processor_thread.start()
        
        # Keep main thread running
        try:
            listener_thread.join()
            processor_thread.join()
        except KeyboardInterrupt:
            print("\nì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

    def generate_response(self, prompt, max_tokens=256):
        """
        Generate response with tokenization handling
        (Same implementation as previous version)
        """
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        full_prompt = f"### æŒ‡ç¤º\n{prompt}\n### å›ç­”\n"
        
        try:
            inputs = self.tokenizer(
                full_prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )
            
            generation_kwargs = {
                'input_ids': inputs.input_ids,
                'attention_mask': inputs.attention_mask,
                'max_new_tokens': max_tokens,
                'do_sample': True,
                'temperature': 0.7,
                'top_p': 0.9,
                'repetition_penalty': 1.2,
                'pad_token_id': self.tokenizer.pad_token_id
            }
            
            outputs = self.model.generate(**generation_kwargs)
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response.split('\n### å›ç­”')[-1].strip()
            
            return response
        
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def main():
    # Get Hugging Face token
    HF_TOKEN = "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"
    
    try:
        # Use Rinna's Japanese GPT-NeoX model
        chatbot = JapaneseChatbot(
            model_id="rinna/japanese-gpt-neox-3.6b", 
            hf_token=HF_TOKEN
        )
        chatbot.start_audio_chat()
    
    except Exception as e:
        print(f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main()