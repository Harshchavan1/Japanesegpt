import os
import sys
import torch
import speech_recognition as sr
import pyttsx3
import threading
import queue
import platform

# Torch and MPS (Metal Performance Shaders) configuration
torch.backends.mps.is_available()
torch.backends.mps.is_built()

from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    GPTNeoXForCausalLM
)

class JapaneseChatbot:
    def __init__(self, model_id="rinna/japanese-gpt-neox-3.6b", hf_token=None):
        """
        Initialize chatbot with M1 Pro optimizations and audio support.
        
        Args:
            model_id (str): Model identifier
            hf_token (str): Hugging Face authentication token
        """
        if not hf_token:
            raise ValueError("Hugging Face í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # Detect device
        self.device = self._select_optimal_device()
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¤‘ì¸ ì¥ì¹˜: {self.device}")

        # Audio setup
        try:
            # Speech Recognition setup
            self.recognizer = sr.Recognizer()
            self.microphone = sr.Microphone()
            
            # Text-to-Speech setup for macOS
            self.tts_engine = self._setup_tts()
            
            # Queue for managing audio input/output
            self.audio_queue = queue.Queue()
            
        except Exception as e:
            print(f"ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            sys.exit(1)

        print("ğŸ–¥ï¸ M1 Pro ìµœì í™” ëª¨ë“œë¡œ ëª¨ë¸ ë¡œë”©...")

        try:
            # Tokenizer and model loading with MPS/Metal optimization
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id, 
                trust_remote_code=True,
                use_fast=False
            )

            # Load model to MPS/Metal device if available
            self.model = GPTNeoXForCausalLM.from_pretrained(
                model_id, 
                torch_dtype=torch.float16 if self.device == 'mps' else torch.float32,
                low_cpu_mem_usage=True
            )
            
            # Move model to optimal device
            self.model = self.model.to(self.device)

            print(f"âœ… {model_id} ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            sys.exit(1)

    def _select_optimal_device(self):
        """
        Select the optimal device for M1 Pro.
        
        Returns:
            str: Device type ('mps', 'cuda', or 'cpu')
        """
        if torch.backends.mps.is_available() and torch.backends.mps.is_built():
            return 'mps'
        elif torch.cuda.is_available():
            return 'cuda'
        return 'cpu'

    def _setup_tts(self):
        """
        Set up Text-to-Speech for macOS with native voices.
        
        Returns:
            pyttsx3.Engine: Configured TTS engine
        """
        tts_engine = pyttsx3.init()
        
        # macOS-specific voice configuration
        tts_engine.setProperty('rate', 150)
        
        # Try to find a Japanese voice on macOS
        voices = tts_engine.getProperty('voices')
        japanese_voices = [
            voice for voice in voices 
            if 'japanese' in voice.name.lower() or 
               'Japan' in voice.name or 
               'ja_JP' in voice.name
        ]
        
        if japanese_voices:
            tts_engine.setProperty('voice', japanese_voices[0].id)
        
        return tts_engine

    def listen_for_audio(self):
        """
        Continuous audio listening thread
        """
        while True:
            try:
                with self.microphone as source:
                    print("ğŸ¤ ë“£ê³  ìˆìŠµë‹ˆë‹¤... (ë§ì”€í•´ ì£¼ì„¸ìš”)")
                    self.recognizer.adjust_for_ambient_noise(source, duration=0.5)
                    audio = self.recognizer.listen(source)
                    
                    # Add audio to queue for processing
                    self.audio_queue.put(audio)
            
            except Exception as e:
                print(f"ìŒì„± ìˆ˜ì‹  ì˜¤ë¥˜: {e}")
                break

    def process_audio_queue(self):
        """
        Process audio queue and generate responses
        """
        while True:
            try:
                # Wait for audio in queue
                audio = self.audio_queue.get()
                
                try:
                    # Recognize speech (Japanese)
                    text = self.recognizer.recognize_google(audio, language='ja-JP')
                    print(f"ğŸ”Š ì¸ì‹ëœ í…ìŠ¤íŠ¸: {text}")
                    
                    # Check for exit commands
                    if text.lower() in ['exit', 'ì¢…ë£Œ', 'ë‚˜ê°€ê¸°', 'quit']:
                        print("ğŸ¤– ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                    
                    # Generate response
                    response = self.generate_response(text)
                    print("ğŸ¤–:", response)
                    
                    # Speak response
                    self.tts_engine.say(response)
                    self.tts_engine.runAndWait()
                
                except sr.UnknownValueError:
                    print("ğŸ¤· ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except sr.RequestError as e:
                    print(f"ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤ ì˜¤ë¥˜: {e}")
            
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def start_audio_chat(self):
        """
        Start continuous audio chat
        """
        print("ğŸ¤– M1 Pro ì¼ë³¸ì–´ ì±—ë´‡: ì•ˆë…•í•˜ì„¸ìš”!")
        print("ğŸ’¡ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ë ¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.")
        print("ğŸ’¡ 'ì¢…ë£Œ' ë˜ëŠ” 'exit'ë¡œ ëŒ€í™” ì¢…ë£Œ ê°€ëŠ¥")

        # Create threads for listening and processing
        listener_thread = threading.Thread(target=self.listen_for_audio, daemon=True)
        processor_thread = threading.Thread(target=self.process_audio_queue, daemon=True)
        
        # Start threads
        listener_thread.start()
        processor_thread.start()
        
        # Keep main thread running
        try:
            listener_thread.join()
            processor_thread.join()
        except KeyboardInterrupt:
            print("\nì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")

    def generate_response(self, prompt, max_tokens=256):
        """
        Generate response with M1 Pro device optimization
        """
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        full_prompt = f"### æŒ‡ç¤º\n{prompt}\n### å›ç­”\n"
        
        try:
            inputs = self.tokenizer(
                full_prompt, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            ).to(self.device)
            
            generation_kwargs = {
                'input_ids': inputs.input_ids,
                'attention_mask': inputs.attention_mask,
                'max_new_tokens': max_tokens,
                'do_sample': True,
                'temperature': 0.7,
                'top_p': 0.9,
                'repetition_penalty': 1.2,
                'pad_token_id': self.tokenizer.pad_token_id
            }
            
            # Generate on the optimal device
            outputs = self.model.generate(**generation_kwargs)
            
            # Move outputs back to CPU for decoding
            response = self.tokenizer.decode(outputs[0].cpu(), skip_special_tokens=True)
            response = response.split('\n### å›ç­”')[-1].strip()
            
            return response
        
        except Exception as e:
            print(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def main():
    # Check if running on macOS
    if platform.system() != 'Darwin':
        print("âŒ ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” macOSì—ì„œë§Œ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # Get Hugging Face token
    HF_TOKEN = "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"
    
    try:
        # Use Rinna's Japanese GPT-NeoX model
        chatbot = JapaneseChatbot(
            model_id="rinna/japanese-gpt-neox-3.6b", 
            hf_token=HF_TOKEN
        )
        chatbot.start_audio_chat()
    
    except Exception as e:
        print(f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main()